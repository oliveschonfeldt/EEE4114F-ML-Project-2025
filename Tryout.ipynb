{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecfd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6305bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_infos():\n",
    "    \"\"\"\n",
    "    Read the file includes data subject information.\n",
    "\n",
    "    EEE4114: Technically we do not need these data, as we are not concerned about identifying the subjects. \n",
    "    \n",
    "    Data Columns:\n",
    "    0: code [1-24]\n",
    "    1: weight [kg]\n",
    "    2: height [cm]\n",
    "    3: age [years]\n",
    "    4: gender [0:Female, 1:Male]\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame that contains information about data subjects' attributes \n",
    "    \"\"\" \n",
    "\n",
    "    dss = pd.read_csv(f'C:/Users/emanu/Documents/GithubRepos/EEE4114F-ML-Project-2025/data/data_subjects_info.csv')\n",
    "    print(\"[INFO] -- Data subjects' information is imported.\")\n",
    "    \n",
    "    return dss\n",
    "\n",
    "def set_data_types(data_types=[\"userAcceleration\"]):\n",
    "    \"\"\"\n",
    "    Select the sensors and the mode to shape the final dataset. \n",
    "\n",
    "    EEE4114F: Choose sensors that you think are useful or would like to include in training. \n",
    "    You can choose all of them, or you could opt to try a limited set of input features\n",
    "    \n",
    "    Args:\n",
    "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n",
    "\n",
    "    Returns:\n",
    "        It returns a list of columns to use for creating time-series from files.\n",
    "    \"\"\"\n",
    "    dt_list = []\n",
    "    for t in data_types:\n",
    "        if t != \"attitude\":\n",
    "            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n",
    "        else:\n",
    "            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
    "\n",
    "    return dt_list\n",
    "\n",
    "\n",
    "def create_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True):\n",
    "    \"\"\"\n",
    "    EEE4114F: This defines what data you would like to include for a given set.  \n",
    "\n",
    "    Args:\n",
    "        dt_list: A list of columns that shows the type of data we want.\n",
    "        act_labels: list of activities.\n",
    "        trial_codes: list of trial codes corresponding to each activity.\n",
    "        mode: It can be \"raw\" (all dimensions of each data type) or \"mag\" (only magnitude per data type).\n",
    "        labeled: True if labeled dataset (adds 'act', 'trial', 'sub_id'), False otherwise.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Time-series sensor data with optional labels.\n",
    "    \"\"\"\n",
    "    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list * 3)\n",
    "\n",
    "    if labeled:\n",
    "        # +3 for act, trial, sub_id labels\n",
    "        dataset = np.zeros((0, num_data_cols + 3))\n",
    "    else:\n",
    "        dataset = np.zeros((0, num_data_cols))\n",
    "\n",
    "    ds_list = get_ds_infos()  # Assume this returns dict with \"code\" key for subject IDs\n",
    "\n",
    "    print(\"[INFO] -- Creating Time-Series\")\n",
    "    for sub_id in ds_list[\"code\"]:\n",
    "        for act_id, act in enumerate(act_labels):\n",
    "            for trial in trial_codes[act_id]:\n",
    "                fname = f'C:/Users/emanu/Documents/GithubRepos/EEE4114F-ML-Project-2025/data/A_DeviceMotion_data/{act}_{trial}/sub_{int(sub_id)}.csv'\n",
    "                raw_data = pd.read_csv(fname)\n",
    "                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
    "                vals = np.zeros((len(raw_data), num_data_cols))\n",
    "                for x_id, axes in enumerate(dt_list):\n",
    "                    if mode == \"mag\":\n",
    "                        # Calculate magnitude of 3 axes per sensor type\n",
    "                        vals[:, x_id] = (raw_data[axes] ** 2).sum(axis=1) ** 0.5        \n",
    "                    else:\n",
    "                        vals[:, x_id * 3:(x_id + 1) * 3] = raw_data[axes].values\n",
    "                    vals = vals[:, :num_data_cols]\n",
    "                if labeled:\n",
    "                    # Add act, trial, sub_id as labels (same length as vals)\n",
    "                    lbls = np.array([[act_id, trial, sub_id]] * len(raw_data))\n",
    "                    vals = np.concatenate((vals, lbls), axis=1)\n",
    "                dataset = np.append(dataset, vals, axis=0)\n",
    "\n",
    "    cols = []\n",
    "    for axes in dt_list:\n",
    "        if mode == \"raw\":\n",
    "            cols += axes\n",
    "        else:\n",
    "            cols += [str(axes[0][:-2])]  # Use first part of sensor name for magnitude\n",
    "\n",
    "    if labeled:\n",
    "        cols += [\"act\", \"trial\", \"sub_id\"]\n",
    "\n",
    "    dataset = pd.DataFrame(data=dataset, columns=cols)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57efa6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Selected sensor data types: ['attitude', 'gravity', 'rotationRate', 'userAcceleration']\n",
      "[INFO] -- Selected activites: ['dws', 'ups', 'wlk', 'jog', 'std', 'sit']\n",
      "[INFO] -- Data subjects' information is imported.\n",
      "[INFO] -- Creating Time-Series\n",
      "[INFO] -- Shape of time-Series dataset:(1412865, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attitude.roll</th>\n",
       "      <th>attitude.pitch</th>\n",
       "      <th>attitude.yaw</th>\n",
       "      <th>gravity.x</th>\n",
       "      <th>gravity.y</th>\n",
       "      <th>gravity.z</th>\n",
       "      <th>rotationRate.x</th>\n",
       "      <th>rotationRate.y</th>\n",
       "      <th>rotationRate.z</th>\n",
       "      <th>userAcceleration.x</th>\n",
       "      <th>userAcceleration.y</th>\n",
       "      <th>userAcceleration.z</th>\n",
       "      <th>act</th>\n",
       "      <th>trial</th>\n",
       "      <th>sub_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.528132</td>\n",
       "      <td>-0.733896</td>\n",
       "      <td>0.696372</td>\n",
       "      <td>0.741895</td>\n",
       "      <td>0.669768</td>\n",
       "      <td>-0.031672</td>\n",
       "      <td>0.316738</td>\n",
       "      <td>0.778180</td>\n",
       "      <td>1.082764</td>\n",
       "      <td>0.294894</td>\n",
       "      <td>-0.184493</td>\n",
       "      <td>0.377542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.527992</td>\n",
       "      <td>-0.716987</td>\n",
       "      <td>0.677762</td>\n",
       "      <td>0.753099</td>\n",
       "      <td>0.657116</td>\n",
       "      <td>-0.032255</td>\n",
       "      <td>0.842032</td>\n",
       "      <td>0.424446</td>\n",
       "      <td>0.643574</td>\n",
       "      <td>0.219405</td>\n",
       "      <td>0.035846</td>\n",
       "      <td>0.114866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.527765</td>\n",
       "      <td>-0.706999</td>\n",
       "      <td>0.670951</td>\n",
       "      <td>0.759611</td>\n",
       "      <td>0.649555</td>\n",
       "      <td>-0.032707</td>\n",
       "      <td>-0.138143</td>\n",
       "      <td>-0.040741</td>\n",
       "      <td>0.343563</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.134701</td>\n",
       "      <td>-0.167808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.516768</td>\n",
       "      <td>-0.704678</td>\n",
       "      <td>0.675735</td>\n",
       "      <td>0.760709</td>\n",
       "      <td>0.647788</td>\n",
       "      <td>-0.041140</td>\n",
       "      <td>-0.025005</td>\n",
       "      <td>-1.048717</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>-0.008389</td>\n",
       "      <td>0.136788</td>\n",
       "      <td>0.094958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.493941</td>\n",
       "      <td>-0.703918</td>\n",
       "      <td>0.672994</td>\n",
       "      <td>0.760062</td>\n",
       "      <td>0.647210</td>\n",
       "      <td>-0.058530</td>\n",
       "      <td>0.114253</td>\n",
       "      <td>-0.912890</td>\n",
       "      <td>0.047341</td>\n",
       "      <td>0.199441</td>\n",
       "      <td>0.353996</td>\n",
       "      <td>-0.044299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attitude.roll  attitude.pitch  attitude.yaw  gravity.x  gravity.y  \\\n",
       "0       1.528132       -0.733896      0.696372   0.741895   0.669768   \n",
       "1       1.527992       -0.716987      0.677762   0.753099   0.657116   \n",
       "2       1.527765       -0.706999      0.670951   0.759611   0.649555   \n",
       "3       1.516768       -0.704678      0.675735   0.760709   0.647788   \n",
       "4       1.493941       -0.703918      0.672994   0.760062   0.647210   \n",
       "\n",
       "   gravity.z  rotationRate.x  rotationRate.y  rotationRate.z  \\\n",
       "0  -0.031672        0.316738        0.778180        1.082764   \n",
       "1  -0.032255        0.842032        0.424446        0.643574   \n",
       "2  -0.032707       -0.138143       -0.040741        0.343563   \n",
       "3  -0.041140       -0.025005       -1.048717        0.035860   \n",
       "4  -0.058530        0.114253       -0.912890        0.047341   \n",
       "\n",
       "   userAcceleration.x  userAcceleration.y  userAcceleration.z  act  trial  \\\n",
       "0            0.294894           -0.184493            0.377542  0.0    1.0   \n",
       "1            0.219405            0.035846            0.114866  0.0    1.0   \n",
       "2            0.010714            0.134701           -0.167808  0.0    1.0   \n",
       "3           -0.008389            0.136788            0.094958  0.0    1.0   \n",
       "4            0.199441            0.353996           -0.044299  0.0    1.0   \n",
       "\n",
       "   sub_id  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
    "TRIAL_CODES = {\n",
    "    ACT_LABELS[0]:[1,2,11],\n",
    "    ACT_LABELS[1]:[3,4,12],\n",
    "    ACT_LABELS[2]:[7,8,15],\n",
    "    ACT_LABELS[3]:[9,16],\n",
    "    ACT_LABELS[4]:[6,14],\n",
    "    ACT_LABELS[5]:[5,13]\n",
    "}\n",
    "\n",
    "## Here we set parameter to build labeled time-series from dataset of \"(A)DeviceMotion_data\"\n",
    "## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\n",
    "sdt = [\"attitude\", \"gravity\", \"rotationRate\", \"userAcceleration\"]\n",
    "print(\"[INFO] -- Selected sensor data types: \"+str(sdt))    \n",
    "act_labels = ACT_LABELS # choose all 6 classes\n",
    "print(\"[INFO] -- Selected activites: \"+str(act_labels))    \n",
    "trial_codes = [TRIAL_CODES[act] for act in act_labels]\n",
    "dt_list = set_data_types(sdt)\n",
    "dataset = create_time_series(dt_list, act_labels, trial_codes, mode=\"raw\", labeled=True)\n",
    "print(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b5044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(dataset, window_size=400, stride=200):\n",
    "    \"\"\"\n",
    "    Slice dataset into overlapping windows of fixed length.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Time-series data with sensor features and 'act' label column.\n",
    "        window_size (int): Number of samples per window.\n",
    "        stride (int): Step size between windows.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with flattened windows and corresponding labels.\n",
    "    \"\"\"\n",
    "    features = dataset.drop(columns=['act']).values\n",
    "    labels = dataset['act'].values\n",
    "\n",
    "    windowed_data = []\n",
    "    windowed_labels = []\n",
    "\n",
    "    for start in range(0, len(dataset) - window_size + 1, stride):\n",
    "        end = start + window_size\n",
    "        window = features[start:end]\n",
    "        label_window = labels[start:end]\n",
    "\n",
    "        # Skip incomplete window (optional)\n",
    "        if len(window) < window_size:\n",
    "            continue\n",
    "\n",
    "        # Flatten window to 1D (e.g., shape becomes [n_features * window_size])\n",
    "        window_flat = window.T.flatten()\n",
    "        # Use the most frequent label in the window as the label\n",
    "        label = np.bincount(label_window.astype(int)).argmax()\n",
    "\n",
    "        windowed_data.append(window_flat)\n",
    "        windowed_labels.append(label)\n",
    "\n",
    "    # Build dataframe\n",
    "    X = pd.DataFrame(windowed_data)\n",
    "    Y = pd.Series(windowed_labels, name='act')\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4a08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionSense(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.X = torch.tensor(X.values)\n",
    "        self.Y = torch.tensor(Y.values)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = int(index.item())\n",
    "\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc9653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split subjects for train/test\n",
    "unique_sub_ids = dataset['sub_id'].unique()\n",
    "train_subs, test_subs = train_test_split(unique_sub_ids, test_size=0.15, random_state=12)\n",
    "\n",
    "train_df = dataset[dataset['sub_id'].isin(train_subs)].reset_index(drop=True)\n",
    "test_df = dataset[dataset['sub_id'].isin(test_subs)].reset_index(drop=True)\n",
    "\n",
    "# Select only sensor features\n",
    "feature_cols = [\n",
    "    'attitude.roll', 'attitude.pitch', 'attitude.yaw',\n",
    "    'gravity.x', 'gravity.y', 'gravity.z',\n",
    "    'rotationRate.x', 'rotationRate.y', 'rotationRate.z',\n",
    "    'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z'\n",
    "]\n",
    "filtered_train_df = train_df[feature_cols + ['act']]\n",
    "filtered_test_df = test_df[feature_cols + ['act']]\n",
    "\n",
    "# Apply windowing\n",
    "X_train, Y_train = windows(filtered_train_df, window_size=400, stride=200)\n",
    "X_test, Y_test = windows(filtered_test_df, window_size=400, stride=200)\n",
    "\n",
    "# 3. Using only training set for CV\n",
    "ts_cv = TimeSeriesSplit(n_splits=5, gap=20, test_size=1000)\n",
    "\n",
    "all_splits = list(ts_cv.split(X_train, Y_train))\n",
    "\n",
    "# # 4. For each split, you get train_idx, val_idx within training data\n",
    "# for train_idx, val_idx in all_splits:\n",
    "#     X_tr = X_train.iloc[train_idx].reset_index(drop=True)\n",
    "#     Y_tr = Y_train.iloc[train_idx].reset_index(drop=True)\n",
    "#     X_val = X_train.iloc[val_idx].reset_index(drop=True)\n",
    "#     Y_val = Y_train.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "#     # Create datasets and loaders for training and validation here\n",
    "#     train_ds = MotionSense(X_tr, Y_tr)\n",
    "#     val_ds = MotionSense(X_val, Y_val)\n",
    "\n",
    "#     train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "#     val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# test_ds = MotionSense(X_test, Y_test)\n",
    "# test_dl = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # Here is an example of the TimeSeriesSplit. It does create cross-validation data by default. \n",
    "# ts_cv = TimeSeriesSplit(\n",
    "#     n_splits=5,\n",
    "#     gap=20,\n",
    "#     #max_train_size=10000,\n",
    "#     test_size=1000\n",
    "# )\n",
    "\n",
    "# all_splits = list(ts_cv.split(X ,Y)) # These are indices, not the data itself.\n",
    "\n",
    "# # Choose the first split for example\n",
    "# train_idx, test_idx = all_splits[0]\n",
    "\n",
    "# X_train = X.iloc[train_idx].reset_index(drop=True)\n",
    "# Y_train = Y.iloc[train_idx].reset_index(drop=True)\n",
    "# X_test = X.iloc[test_idx].reset_index(drop=True)\n",
    "# Y_test = Y.iloc[test_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ecf0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d911b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = MotionSense(X_train, Y_train)\n",
    "# test_dataset = MotionSense(X_test, Y_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1deb3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the improved fully connected network\n",
    "# class FCNet(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(FCNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "#         self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "#         self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "#         self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "#         self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)  # Flatten input\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout1(x)\n",
    "        \n",
    "#         x = self.fc2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout2(x)\n",
    "        \n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# # Parameters - adjust based on your dataset's window size and features\n",
    "# input_size = 12 * 400  # 6 sensors * 400 time steps\n",
    "# hidden_size = 200     # Increased for better representation\n",
    "# num_classes = 6       # Number of activity classes\n",
    "\n",
    "# # Initialize model, loss, and optimizer with weight decay for regularization\n",
    "# model = FCNet(input_size, hidden_size, num_classes)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# # Example training loop\n",
    "# num_epochs = 20  # Extended training\n",
    "\n",
    "# train_losses = []\n",
    "# train_accuracies = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs.float())\n",
    "#         loss = criterion(outputs, labels.long())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     epoch_acc = 100 * correct / total\n",
    "\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     train_accuracies.append(epoch_acc)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ef9dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the improved fully connected network\n",
    "class FCNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Parameters - adjust based on your dataset's window size and features\n",
    "input_size = 12 * 400  # 6 sensors * 400 time steps\n",
    "hidden_size = 100     # Increased for better representation\n",
    "num_classes = 6       # Number of activity classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90320b75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1: Train Loss=1.3445, Train Acc=52.96%, Val Acc=46.50%\n",
      "Epoch 2: Train Loss=0.9815, Train Acc=70.25%, Val Acc=50.20%\n",
      "Epoch 3: Train Loss=0.7648, Train Acc=79.81%, Val Acc=52.00%\n",
      "Epoch 4: Train Loss=0.5792, Train Acc=88.88%, Val Acc=59.60%\n",
      "Epoch 5: Train Loss=0.4210, Train Acc=93.35%, Val Acc=60.50%\n",
      "Epoch 6: Train Loss=0.3065, Train Acc=96.86%, Val Acc=62.20%\n",
      "Epoch 7: Train Loss=0.2472, Train Acc=96.74%, Val Acc=67.80%\n",
      "Epoch 8: Train Loss=0.1984, Train Acc=97.58%, Val Acc=57.90%\n",
      "Epoch 9: Train Loss=0.1595, Train Acc=98.19%, Val Acc=60.90%\n",
      "Epoch 10: Train Loss=0.1473, Train Acc=98.31%, Val Acc=68.00%\n",
      "Epoch 11: Train Loss=0.1178, Train Acc=98.19%, Val Acc=68.70%\n",
      "Epoch 12: Train Loss=0.1051, Train Acc=98.31%, Val Acc=68.40%\n",
      "Epoch 13: Train Loss=0.0924, Train Acc=98.43%, Val Acc=64.40%\n",
      "Epoch 14: Train Loss=0.0860, Train Acc=98.55%, Val Acc=68.10%\n",
      "Epoch 15: Train Loss=0.0724, Train Acc=98.91%, Val Acc=65.40%\n",
      "Epoch 16: Train Loss=0.0695, Train Acc=99.03%, Val Acc=64.80%\n",
      "Epoch 17: Train Loss=0.0527, Train Acc=99.88%, Val Acc=65.60%\n",
      "Epoch 18: Train Loss=0.0445, Train Acc=99.76%, Val Acc=66.30%\n",
      "Epoch 19: Train Loss=0.0437, Train Acc=99.52%, Val Acc=68.50%\n",
      "Epoch 20: Train Loss=0.0427, Train Acc=99.40%, Val Acc=65.70%\n",
      "Epoch 21: Train Loss=0.0515, Train Acc=99.15%, Val Acc=63.60%\n",
      "Epoch 22: Train Loss=0.0379, Train Acc=99.76%, Val Acc=65.40%\n",
      "Epoch 23: Train Loss=0.0446, Train Acc=99.15%, Val Acc=64.50%\n",
      "Epoch 24: Train Loss=0.0545, Train Acc=98.55%, Val Acc=63.60%\n",
      "Epoch 25: Train Loss=0.0628, Train Acc=98.07%, Val Acc=65.30%\n",
      "Epoch 26: Train Loss=0.0632, Train Acc=98.55%, Val Acc=61.30%\n",
      "Epoch 27: Train Loss=0.0565, Train Acc=99.03%, Val Acc=64.30%\n",
      "Epoch 28: Train Loss=0.0358, Train Acc=99.64%, Val Acc=64.40%\n",
      "Epoch 29: Train Loss=0.0406, Train Acc=99.27%, Val Acc=63.30%\n",
      "Epoch 30: Train Loss=0.0249, Train Acc=99.76%, Val Acc=66.90%\n",
      "Epoch 31: Train Loss=0.0236, Train Acc=99.52%, Val Acc=67.30%\n",
      "Epoch 32: Train Loss=0.0398, Train Acc=99.27%, Val Acc=66.50%\n",
      "Epoch 33: Train Loss=0.0212, Train Acc=100.00%, Val Acc=66.20%\n",
      "Epoch 34: Train Loss=0.0215, Train Acc=99.64%, Val Acc=64.80%\n",
      "Epoch 35: Train Loss=0.0245, Train Acc=99.40%, Val Acc=64.90%\n",
      "Epoch 36: Train Loss=0.0210, Train Acc=99.64%, Val Acc=67.80%\n",
      "Epoch 37: Train Loss=0.0209, Train Acc=99.76%, Val Acc=61.70%\n",
      "Epoch 38: Train Loss=0.0188, Train Acc=100.00%, Val Acc=65.00%\n",
      "Epoch 39: Train Loss=0.0230, Train Acc=99.52%, Val Acc=64.90%\n",
      "Epoch 40: Train Loss=0.0191, Train Acc=99.64%, Val Acc=64.80%\n",
      "Epoch 41: Train Loss=0.0287, Train Acc=99.76%, Val Acc=67.60%\n",
      "Epoch 42: Train Loss=0.0269, Train Acc=99.52%, Val Acc=68.40%\n",
      "Epoch 43: Train Loss=0.0376, Train Acc=98.91%, Val Acc=60.40%\n",
      "Epoch 44: Train Loss=0.0278, Train Acc=99.27%, Val Acc=65.30%\n",
      "Epoch 45: Train Loss=0.0391, Train Acc=99.27%, Val Acc=65.10%\n",
      "Epoch 46: Train Loss=0.0317, Train Acc=99.27%, Val Acc=63.60%\n",
      "Epoch 47: Train Loss=0.0195, Train Acc=99.64%, Val Acc=64.50%\n",
      "Epoch 48: Train Loss=0.0172, Train Acc=99.88%, Val Acc=65.10%\n",
      "Epoch 49: Train Loss=0.0205, Train Acc=99.27%, Val Acc=67.10%\n",
      "Epoch 50: Train Loss=0.0321, Train Acc=99.15%, Val Acc=64.60%\n",
      "Fold 2\n",
      "Epoch 1: Train Loss=1.2528, Train Acc=55.56%, Val Acc=61.70%\n",
      "Epoch 2: Train Loss=0.8176, Train Acc=77.50%, Val Acc=70.30%\n",
      "Epoch 3: Train Loss=0.5667, Train Acc=88.18%, Val Acc=75.00%\n",
      "Epoch 4: Train Loss=0.3662, Train Acc=92.88%, Val Acc=76.30%\n",
      "Epoch 5: Train Loss=0.3291, Train Acc=92.01%, Val Acc=76.40%\n",
      "Epoch 6: Train Loss=0.2538, Train Acc=94.58%, Val Acc=76.70%\n",
      "Epoch 7: Train Loss=0.1859, Train Acc=95.73%, Val Acc=78.20%\n",
      "Epoch 8: Train Loss=0.1546, Train Acc=96.33%, Val Acc=78.20%\n",
      "Epoch 9: Train Loss=0.1272, Train Acc=97.70%, Val Acc=77.40%\n",
      "Epoch 10: Train Loss=0.1369, Train Acc=96.93%, Val Acc=78.70%\n",
      "Epoch 11: Train Loss=0.1315, Train Acc=96.61%, Val Acc=80.80%\n",
      "Epoch 12: Train Loss=0.1485, Train Acc=96.17%, Val Acc=77.90%\n",
      "Epoch 13: Train Loss=0.1682, Train Acc=96.28%, Val Acc=80.60%\n",
      "Epoch 14: Train Loss=0.1176, Train Acc=96.93%, Val Acc=82.20%\n",
      "Epoch 15: Train Loss=0.1414, Train Acc=97.43%, Val Acc=80.80%\n",
      "Epoch 16: Train Loss=0.1765, Train Acc=95.73%, Val Acc=77.50%\n",
      "Epoch 17: Train Loss=0.1102, Train Acc=96.83%, Val Acc=79.20%\n",
      "Epoch 18: Train Loss=0.1200, Train Acc=96.99%, Val Acc=79.80%\n",
      "Epoch 19: Train Loss=0.1117, Train Acc=96.99%, Val Acc=77.10%\n",
      "Epoch 20: Train Loss=0.1049, Train Acc=97.26%, Val Acc=79.10%\n",
      "Epoch 21: Train Loss=0.1207, Train Acc=98.30%, Val Acc=79.00%\n",
      "Epoch 22: Train Loss=0.1230, Train Acc=96.99%, Val Acc=80.50%\n",
      "Epoch 23: Train Loss=0.0981, Train Acc=97.92%, Val Acc=80.70%\n",
      "Epoch 24: Train Loss=0.1321, Train Acc=97.10%, Val Acc=77.60%\n",
      "Epoch 25: Train Loss=0.0902, Train Acc=97.59%, Val Acc=77.60%\n",
      "Epoch 26: Train Loss=0.1268, Train Acc=97.59%, Val Acc=76.50%\n",
      "Epoch 27: Train Loss=0.0810, Train Acc=97.76%, Val Acc=76.40%\n",
      "Epoch 28: Train Loss=0.0923, Train Acc=97.70%, Val Acc=77.10%\n",
      "Epoch 29: Train Loss=0.1894, Train Acc=97.87%, Val Acc=76.00%\n",
      "Epoch 30: Train Loss=0.1550, Train Acc=95.73%, Val Acc=72.90%\n",
      "Epoch 31: Train Loss=0.1184, Train Acc=96.28%, Val Acc=75.30%\n",
      "Epoch 32: Train Loss=0.2204, Train Acc=97.15%, Val Acc=77.10%\n",
      "Epoch 33: Train Loss=0.0947, Train Acc=97.59%, Val Acc=76.90%\n",
      "Epoch 34: Train Loss=0.1025, Train Acc=98.41%, Val Acc=77.20%\n",
      "Epoch 35: Train Loss=0.0861, Train Acc=97.26%, Val Acc=78.10%\n",
      "Epoch 36: Train Loss=0.0657, Train Acc=98.30%, Val Acc=76.60%\n",
      "Epoch 37: Train Loss=0.0619, Train Acc=98.30%, Val Acc=79.10%\n",
      "Epoch 38: Train Loss=0.0536, Train Acc=98.74%, Val Acc=78.20%\n",
      "Epoch 39: Train Loss=0.0633, Train Acc=97.87%, Val Acc=77.90%\n",
      "Epoch 40: Train Loss=0.0524, Train Acc=98.25%, Val Acc=77.40%\n",
      "Epoch 41: Train Loss=0.1015, Train Acc=98.19%, Val Acc=78.20%\n",
      "Epoch 42: Train Loss=0.0570, Train Acc=97.97%, Val Acc=77.10%\n",
      "Epoch 43: Train Loss=0.0747, Train Acc=98.80%, Val Acc=76.60%\n",
      "Epoch 44: Train Loss=0.0993, Train Acc=98.14%, Val Acc=77.80%\n",
      "Epoch 45: Train Loss=0.0710, Train Acc=97.70%, Val Acc=77.70%\n",
      "Epoch 46: Train Loss=0.0427, Train Acc=98.63%, Val Acc=78.80%\n",
      "Epoch 47: Train Loss=0.0563, Train Acc=98.14%, Val Acc=80.30%\n",
      "Epoch 48: Train Loss=0.0735, Train Acc=98.74%, Val Acc=78.30%\n",
      "Epoch 49: Train Loss=0.0809, Train Acc=98.19%, Val Acc=76.70%\n",
      "Epoch 50: Train Loss=0.0989, Train Acc=97.15%, Val Acc=77.60%\n",
      "Fold 3\n",
      "Epoch 1: Train Loss=1.1826, Train Acc=60.03%, Val Acc=71.00%\n",
      "Epoch 2: Train Loss=0.7052, Train Acc=79.84%, Val Acc=77.20%\n",
      "Epoch 3: Train Loss=0.4678, Train Acc=87.48%, Val Acc=88.70%\n",
      "Epoch 4: Train Loss=0.3185, Train Acc=92.29%, Val Acc=91.00%\n",
      "Epoch 5: Train Loss=0.2430, Train Acc=94.13%, Val Acc=95.00%\n",
      "Epoch 6: Train Loss=0.1970, Train Acc=94.84%, Val Acc=92.10%\n",
      "Epoch 7: Train Loss=0.2007, Train Acc=94.48%, Val Acc=89.70%\n",
      "Epoch 8: Train Loss=0.1496, Train Acc=96.43%, Val Acc=94.20%\n",
      "Epoch 9: Train Loss=0.1623, Train Acc=95.40%, Val Acc=91.40%\n",
      "Epoch 10: Train Loss=0.1355, Train Acc=96.43%, Val Acc=91.40%\n",
      "Epoch 11: Train Loss=0.1123, Train Acc=96.71%, Val Acc=86.70%\n",
      "Epoch 12: Train Loss=0.0892, Train Acc=97.63%, Val Acc=91.60%\n",
      "Epoch 13: Train Loss=0.0826, Train Acc=97.74%, Val Acc=95.00%\n",
      "Epoch 14: Train Loss=0.0792, Train Acc=98.09%, Val Acc=94.40%\n",
      "Epoch 15: Train Loss=0.0758, Train Acc=97.84%, Val Acc=92.30%\n",
      "Epoch 16: Train Loss=0.0641, Train Acc=98.16%, Val Acc=87.30%\n",
      "Epoch 17: Train Loss=0.0835, Train Acc=97.63%, Val Acc=93.80%\n",
      "Epoch 18: Train Loss=0.0865, Train Acc=97.42%, Val Acc=94.40%\n",
      "Epoch 19: Train Loss=0.0859, Train Acc=97.59%, Val Acc=92.60%\n",
      "Epoch 20: Train Loss=0.0731, Train Acc=97.91%, Val Acc=90.20%\n",
      "Epoch 21: Train Loss=0.0647, Train Acc=98.23%, Val Acc=92.50%\n",
      "Epoch 22: Train Loss=0.0673, Train Acc=97.88%, Val Acc=92.70%\n",
      "Epoch 23: Train Loss=0.0643, Train Acc=98.37%, Val Acc=94.20%\n",
      "Epoch 24: Train Loss=0.0604, Train Acc=98.09%, Val Acc=93.00%\n",
      "Epoch 25: Train Loss=0.0555, Train Acc=98.09%, Val Acc=88.80%\n",
      "Epoch 26: Train Loss=0.0458, Train Acc=98.62%, Val Acc=94.60%\n",
      "Epoch 27: Train Loss=0.0517, Train Acc=98.23%, Val Acc=95.30%\n",
      "Epoch 28: Train Loss=0.0546, Train Acc=98.44%, Val Acc=94.50%\n",
      "Epoch 29: Train Loss=0.0402, Train Acc=98.62%, Val Acc=93.60%\n",
      "Epoch 30: Train Loss=0.0356, Train Acc=98.83%, Val Acc=93.80%\n",
      "Epoch 31: Train Loss=0.0772, Train Acc=98.09%, Val Acc=94.60%\n",
      "Epoch 32: Train Loss=0.0516, Train Acc=98.44%, Val Acc=93.30%\n",
      "Epoch 33: Train Loss=0.0613, Train Acc=97.98%, Val Acc=94.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss=0.0338, Train Acc=98.94%, Val Acc=92.70%\n",
      "Epoch 35: Train Loss=0.0370, Train Acc=99.01%, Val Acc=91.40%\n",
      "Epoch 36: Train Loss=0.0392, Train Acc=99.08%, Val Acc=94.90%\n",
      "Epoch 37: Train Loss=0.0410, Train Acc=98.87%, Val Acc=95.00%\n",
      "Epoch 38: Train Loss=0.0289, Train Acc=99.29%, Val Acc=85.80%\n",
      "Epoch 39: Train Loss=0.0264, Train Acc=99.22%, Val Acc=91.50%\n",
      "Epoch 40: Train Loss=0.0291, Train Acc=99.22%, Val Acc=93.00%\n",
      "Epoch 41: Train Loss=0.0378, Train Acc=98.83%, Val Acc=92.80%\n",
      "Epoch 42: Train Loss=0.0326, Train Acc=98.90%, Val Acc=92.80%\n",
      "Epoch 43: Train Loss=0.0409, Train Acc=98.73%, Val Acc=90.40%\n",
      "Epoch 44: Train Loss=0.0360, Train Acc=99.01%, Val Acc=95.00%\n",
      "Epoch 45: Train Loss=0.0552, Train Acc=98.66%, Val Acc=92.10%\n",
      "Epoch 46: Train Loss=0.0605, Train Acc=98.27%, Val Acc=92.60%\n",
      "Epoch 47: Train Loss=0.0349, Train Acc=98.76%, Val Acc=91.30%\n",
      "Epoch 48: Train Loss=0.0405, Train Acc=98.76%, Val Acc=92.20%\n",
      "Epoch 49: Train Loss=0.0302, Train Acc=99.12%, Val Acc=93.00%\n",
      "Epoch 50: Train Loss=0.0278, Train Acc=99.19%, Val Acc=94.70%\n",
      "Fold 4\n",
      "Epoch 1: Train Loss=1.1304, Train Acc=62.09%, Val Acc=82.50%\n",
      "Epoch 2: Train Loss=0.6037, Train Acc=83.96%, Val Acc=85.90%\n",
      "Epoch 3: Train Loss=0.3676, Train Acc=91.51%, Val Acc=89.00%\n",
      "Epoch 4: Train Loss=0.2752, Train Acc=92.87%, Val Acc=88.90%\n",
      "Epoch 5: Train Loss=0.2258, Train Acc=94.38%, Val Acc=89.70%\n",
      "Epoch 6: Train Loss=0.1690, Train Acc=95.56%, Val Acc=88.30%\n",
      "Epoch 7: Train Loss=0.1423, Train Acc=96.13%, Val Acc=86.20%\n",
      "Epoch 8: Train Loss=0.1514, Train Acc=95.85%, Val Acc=89.30%\n",
      "Epoch 9: Train Loss=0.1160, Train Acc=96.60%, Val Acc=90.00%\n",
      "Epoch 10: Train Loss=0.0990, Train Acc=97.10%, Val Acc=90.20%\n",
      "Epoch 11: Train Loss=0.1065, Train Acc=97.00%, Val Acc=90.70%\n",
      "Epoch 12: Train Loss=0.0969, Train Acc=97.13%, Val Acc=90.20%\n",
      "Epoch 13: Train Loss=0.0909, Train Acc=97.31%, Val Acc=88.70%\n",
      "Epoch 14: Train Loss=0.0814, Train Acc=97.62%, Val Acc=89.90%\n",
      "Epoch 15: Train Loss=0.0743, Train Acc=97.81%, Val Acc=86.50%\n",
      "Epoch 16: Train Loss=0.0694, Train Acc=97.91%, Val Acc=91.10%\n",
      "Epoch 17: Train Loss=0.0888, Train Acc=97.02%, Val Acc=88.40%\n",
      "Epoch 18: Train Loss=0.0639, Train Acc=98.04%, Val Acc=89.40%\n",
      "Epoch 19: Train Loss=0.0704, Train Acc=97.81%, Val Acc=90.40%\n",
      "Epoch 20: Train Loss=0.0560, Train Acc=98.30%, Val Acc=87.50%\n",
      "Epoch 21: Train Loss=0.0559, Train Acc=98.33%, Val Acc=89.70%\n",
      "Epoch 22: Train Loss=0.0520, Train Acc=98.62%, Val Acc=89.90%\n",
      "Epoch 23: Train Loss=0.0611, Train Acc=97.88%, Val Acc=90.00%\n",
      "Epoch 24: Train Loss=0.0600, Train Acc=97.99%, Val Acc=87.90%\n",
      "Epoch 25: Train Loss=0.0565, Train Acc=98.28%, Val Acc=90.80%\n",
      "Epoch 26: Train Loss=0.0464, Train Acc=98.54%, Val Acc=87.50%\n",
      "Epoch 27: Train Loss=0.0389, Train Acc=98.69%, Val Acc=89.30%\n",
      "Epoch 28: Train Loss=0.0406, Train Acc=98.75%, Val Acc=87.40%\n",
      "Epoch 29: Train Loss=0.0350, Train Acc=99.09%, Val Acc=88.90%\n",
      "Epoch 30: Train Loss=0.0393, Train Acc=98.75%, Val Acc=88.40%\n",
      "Epoch 31: Train Loss=0.0591, Train Acc=98.25%, Val Acc=89.60%\n",
      "Epoch 32: Train Loss=0.0456, Train Acc=98.72%, Val Acc=87.40%\n",
      "Epoch 33: Train Loss=0.0478, Train Acc=98.41%, Val Acc=88.30%\n",
      "Epoch 34: Train Loss=0.0517, Train Acc=98.38%, Val Acc=91.50%\n",
      "Epoch 35: Train Loss=0.0526, Train Acc=98.51%, Val Acc=88.50%\n",
      "Epoch 36: Train Loss=0.0500, Train Acc=98.22%, Val Acc=89.90%\n",
      "Epoch 37: Train Loss=0.0433, Train Acc=98.38%, Val Acc=88.80%\n",
      "Epoch 38: Train Loss=0.0269, Train Acc=99.22%, Val Acc=88.40%\n",
      "Epoch 39: Train Loss=0.0357, Train Acc=99.03%, Val Acc=90.30%\n",
      "Epoch 40: Train Loss=0.0330, Train Acc=99.09%, Val Acc=91.70%\n",
      "Epoch 41: Train Loss=0.0401, Train Acc=98.90%, Val Acc=90.00%\n",
      "Epoch 42: Train Loss=0.0299, Train Acc=99.06%, Val Acc=91.10%\n",
      "Epoch 43: Train Loss=0.0246, Train Acc=99.32%, Val Acc=90.10%\n",
      "Epoch 44: Train Loss=0.0369, Train Acc=98.82%, Val Acc=90.50%\n",
      "Epoch 45: Train Loss=0.0345, Train Acc=98.77%, Val Acc=90.20%\n",
      "Epoch 46: Train Loss=0.0337, Train Acc=98.93%, Val Acc=90.70%\n",
      "Epoch 47: Train Loss=0.0330, Train Acc=98.90%, Val Acc=89.10%\n",
      "Epoch 48: Train Loss=0.0376, Train Acc=98.69%, Val Acc=89.70%\n",
      "Epoch 49: Train Loss=0.0303, Train Acc=99.11%, Val Acc=87.80%\n",
      "Epoch 50: Train Loss=0.0456, Train Acc=98.69%, Val Acc=90.60%\n",
      "Fold 5\n",
      "Epoch 1: Train Loss=1.0285, Train Acc=66.23%, Val Acc=91.50%\n",
      "Epoch 2: Train Loss=0.5049, Train Acc=86.91%, Val Acc=93.30%\n",
      "Epoch 3: Train Loss=0.3354, Train Acc=91.15%, Val Acc=92.80%\n",
      "Epoch 4: Train Loss=0.2494, Train Acc=93.35%, Val Acc=94.60%\n",
      "Epoch 5: Train Loss=0.1981, Train Acc=94.43%, Val Acc=95.80%\n",
      "Epoch 6: Train Loss=0.1666, Train Acc=95.57%, Val Acc=93.60%\n",
      "Epoch 7: Train Loss=0.1431, Train Acc=96.08%, Val Acc=95.10%\n",
      "Epoch 8: Train Loss=0.1288, Train Acc=96.21%, Val Acc=94.40%\n",
      "Epoch 9: Train Loss=0.1297, Train Acc=96.15%, Val Acc=94.30%\n",
      "Epoch 10: Train Loss=0.1135, Train Acc=96.87%, Val Acc=95.20%\n",
      "Epoch 11: Train Loss=0.1165, Train Acc=96.69%, Val Acc=94.40%\n",
      "Epoch 12: Train Loss=0.1015, Train Acc=96.85%, Val Acc=95.50%\n",
      "Epoch 13: Train Loss=0.0979, Train Acc=97.14%, Val Acc=95.20%\n",
      "Epoch 14: Train Loss=0.0883, Train Acc=97.27%, Val Acc=95.20%\n",
      "Epoch 15: Train Loss=0.0696, Train Acc=98.16%, Val Acc=95.50%\n",
      "Epoch 16: Train Loss=0.0808, Train Acc=97.51%, Val Acc=95.40%\n",
      "Epoch 17: Train Loss=0.0656, Train Acc=97.97%, Val Acc=95.00%\n",
      "Epoch 18: Train Loss=0.0717, Train Acc=97.99%, Val Acc=94.70%\n",
      "Epoch 19: Train Loss=0.0726, Train Acc=97.82%, Val Acc=94.80%\n",
      "Epoch 20: Train Loss=0.0632, Train Acc=98.03%, Val Acc=94.90%\n",
      "Epoch 21: Train Loss=0.0612, Train Acc=98.18%, Val Acc=95.00%\n",
      "Epoch 22: Train Loss=0.0656, Train Acc=98.03%, Val Acc=95.30%\n",
      "Epoch 23: Train Loss=0.0483, Train Acc=98.59%, Val Acc=94.80%\n",
      "Epoch 24: Train Loss=0.0573, Train Acc=98.14%, Val Acc=95.10%\n",
      "Epoch 25: Train Loss=0.0655, Train Acc=97.91%, Val Acc=95.70%\n",
      "Epoch 26: Train Loss=0.0626, Train Acc=98.09%, Val Acc=94.20%\n",
      "Epoch 27: Train Loss=0.0540, Train Acc=98.09%, Val Acc=95.20%\n",
      "Epoch 28: Train Loss=0.0483, Train Acc=98.74%, Val Acc=95.90%\n",
      "Epoch 29: Train Loss=0.0466, Train Acc=98.69%, Val Acc=95.20%\n",
      "Epoch 30: Train Loss=0.0544, Train Acc=98.40%, Val Acc=95.60%\n",
      "Epoch 31: Train Loss=0.0516, Train Acc=98.40%, Val Acc=96.00%\n",
      "Epoch 32: Train Loss=0.0423, Train Acc=98.74%, Val Acc=96.10%\n",
      "Epoch 33: Train Loss=0.0528, Train Acc=98.49%, Val Acc=95.80%\n",
      "Epoch 34: Train Loss=0.0476, Train Acc=98.51%, Val Acc=95.90%\n",
      "Epoch 35: Train Loss=0.0390, Train Acc=98.82%, Val Acc=95.10%\n",
      "Epoch 36: Train Loss=0.0407, Train Acc=98.59%, Val Acc=96.40%\n",
      "Epoch 37: Train Loss=0.0436, Train Acc=98.63%, Val Acc=95.20%\n",
      "Epoch 38: Train Loss=0.0453, Train Acc=98.67%, Val Acc=95.90%\n",
      "Epoch 39: Train Loss=0.0407, Train Acc=98.90%, Val Acc=96.10%\n",
      "Epoch 40: Train Loss=0.0451, Train Acc=98.80%, Val Acc=94.80%\n",
      "Epoch 41: Train Loss=0.0327, Train Acc=98.98%, Val Acc=96.00%\n",
      "Epoch 42: Train Loss=0.0362, Train Acc=98.90%, Val Acc=94.80%\n",
      "Epoch 43: Train Loss=0.0311, Train Acc=99.09%, Val Acc=96.00%\n",
      "Epoch 44: Train Loss=0.0244, Train Acc=99.32%, Val Acc=95.80%\n",
      "Epoch 45: Train Loss=0.0350, Train Acc=98.82%, Val Acc=95.40%\n",
      "Epoch 46: Train Loss=0.0342, Train Acc=98.92%, Val Acc=95.80%\n",
      "Epoch 47: Train Loss=0.0360, Train Acc=98.92%, Val Acc=95.30%\n",
      "Epoch 48: Train Loss=0.0421, Train Acc=98.65%, Val Acc=95.40%\n",
      "Epoch 49: Train Loss=0.0379, Train Acc=98.69%, Val Acc=94.90%\n",
      "Epoch 50: Train Loss=0.0397, Train Acc=98.84%, Val Acc=95.60%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(all_splits):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    X_tr = X_train.iloc[train_idx].reset_index(drop=True)\n",
    "    Y_tr = Y_train.iloc[train_idx].reset_index(drop=True)\n",
    "    X_val = X_train.iloc[val_idx].reset_index(drop=True)\n",
    "    Y_val = Y_train.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_ds = MotionSense(X_tr, Y_tr)\n",
    "    val_ds = MotionSense(X_val, Y_val)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = FCNet(input_size, hidden_size, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_dl)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dl:\n",
    "                outputs = model(inputs.float())\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "    # Track best validation accuracy to select the best fold model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "267fd12f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy from CV: 95.60%\n",
      "Retraining on full training data...\n",
      "Retrain Epoch 1: Loss = 0.0891, Accuracy = 97.66%\n",
      "Retrain Epoch 2: Loss = 0.0561, Accuracy = 98.41%\n",
      "Retrain Epoch 3: Loss = 0.0484, Accuracy = 98.46%\n",
      "Retrain Epoch 4: Loss = 0.0504, Accuracy = 98.48%\n",
      "Retrain Epoch 5: Loss = 0.0380, Accuracy = 98.82%\n",
      "Retrain Epoch 6: Loss = 0.0355, Accuracy = 98.87%\n",
      "Retrain Epoch 7: Loss = 0.0419, Accuracy = 98.73%\n",
      "Retrain Epoch 8: Loss = 0.0330, Accuracy = 99.03%\n",
      "Retrain Epoch 9: Loss = 0.0342, Accuracy = 99.06%\n",
      "Retrain Epoch 10: Loss = 0.0369, Accuracy = 98.75%\n",
      "Retrain Epoch 11: Loss = 0.0362, Accuracy = 98.87%\n",
      "Retrain Epoch 12: Loss = 0.0325, Accuracy = 99.04%\n",
      "Retrain Epoch 13: Loss = 0.0365, Accuracy = 98.92%\n",
      "Retrain Epoch 14: Loss = 0.0294, Accuracy = 99.14%\n",
      "Retrain Epoch 15: Loss = 0.0361, Accuracy = 98.84%\n",
      "Retrain Epoch 16: Loss = 0.0313, Accuracy = 99.01%\n",
      "Retrain Epoch 17: Loss = 0.0332, Accuracy = 98.84%\n",
      "Retrain Epoch 18: Loss = 0.0284, Accuracy = 99.16%\n",
      "Retrain Epoch 19: Loss = 0.0270, Accuracy = 99.09%\n",
      "Retrain Epoch 20: Loss = 0.0394, Accuracy = 98.91%\n",
      "Retrain Epoch 21: Loss = 0.0331, Accuracy = 99.04%\n",
      "Retrain Epoch 22: Loss = 0.0281, Accuracy = 99.13%\n",
      "Retrain Epoch 23: Loss = 0.0233, Accuracy = 99.23%\n",
      "Retrain Epoch 24: Loss = 0.0325, Accuracy = 98.94%\n",
      "Retrain Epoch 25: Loss = 0.0333, Accuracy = 98.92%\n",
      "Retrain Epoch 26: Loss = 0.0210, Accuracy = 99.30%\n",
      "Retrain Epoch 27: Loss = 0.0216, Accuracy = 99.32%\n",
      "Retrain Epoch 28: Loss = 0.0326, Accuracy = 98.92%\n",
      "Retrain Epoch 29: Loss = 0.0362, Accuracy = 98.91%\n",
      "Retrain Epoch 30: Loss = 0.0338, Accuracy = 98.80%\n",
      "Retrain Epoch 31: Loss = 0.0273, Accuracy = 99.11%\n",
      "Retrain Epoch 32: Loss = 0.0300, Accuracy = 99.11%\n",
      "Retrain Epoch 33: Loss = 0.0271, Accuracy = 99.20%\n",
      "Retrain Epoch 34: Loss = 0.0305, Accuracy = 99.03%\n",
      "Retrain Epoch 35: Loss = 0.0266, Accuracy = 99.09%\n",
      "Retrain Epoch 36: Loss = 0.0301, Accuracy = 99.08%\n",
      "Retrain Epoch 37: Loss = 0.0223, Accuracy = 99.37%\n",
      "Retrain Epoch 38: Loss = 0.0281, Accuracy = 99.04%\n",
      "Retrain Epoch 39: Loss = 0.0203, Accuracy = 99.25%\n",
      "Retrain Epoch 40: Loss = 0.0237, Accuracy = 99.25%\n",
      "Retrain Epoch 41: Loss = 0.0314, Accuracy = 99.16%\n",
      "Retrain Epoch 42: Loss = 0.0264, Accuracy = 99.16%\n",
      "Retrain Epoch 43: Loss = 0.0223, Accuracy = 99.38%\n",
      "Retrain Epoch 44: Loss = 0.0235, Accuracy = 99.32%\n",
      "Retrain Epoch 45: Loss = 0.0359, Accuracy = 98.85%\n",
      "Retrain Epoch 46: Loss = 0.0259, Accuracy = 99.18%\n",
      "Retrain Epoch 47: Loss = 0.0183, Accuracy = 99.50%\n",
      "Retrain Epoch 48: Loss = 0.0200, Accuracy = 99.32%\n",
      "Retrain Epoch 49: Loss = 0.0362, Accuracy = 98.99%\n",
      "Retrain Epoch 50: Loss = 0.0272, Accuracy = 99.04%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best validation accuracy from CV: {best_val_acc:.2f}%\")\n",
    "print(\"Retraining on full training data...\")\n",
    "\n",
    "full_train_ds = MotionSense(X_train, Y_train)\n",
    "full_train_dl = DataLoader(full_train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "model = FCNet(input_size, hidden_size, num_classes)\n",
    "model.load_state_dict(best_model_state)  # Optionally initialize from best fold\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in full_train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(full_train_dl)\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Retrain Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8712b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.09%\n"
     ]
    }
   ],
   "source": [
    "test_ds = MotionSense(X_test, Y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl:\n",
    "        outputs = model(inputs.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763caa58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad37ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
