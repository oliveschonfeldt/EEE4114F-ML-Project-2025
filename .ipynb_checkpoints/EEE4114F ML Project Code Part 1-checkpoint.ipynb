{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129fbeb3",
   "metadata": {},
   "source": [
    "# EEE4114F ML Project Code Part 1: Loading and Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3021b42",
   "metadata": {},
   "source": [
    "The first step in building our human activity recognition (HAR) classifier was to determine which subset of the available data to use. The dataset comprises multiple files and sensor modalities, but it is neither efficient nor always beneficial to use all of them. Therefore, we evaluated which sensor signals were likely to contribute most meaningfully to classification accuracy, while balancing computational cost and potential overfitting.\n",
    "\n",
    "The dataset is divided into three main folders:\n",
    "\n",
    "- **(A) DeviceMotion_data:** Contains a rich combination of accelerometer, gyroscope, and orientation-related features — 12 features in total.\n",
    "\n",
    "- **(B) Accelerometer_data:** Includes only the raw accelerometer readings (x, y, z) — 3 features.\n",
    "\n",
    "- **(C) Gyroscope_data:** Includes only the raw gyroscope readings (x, y, z) — 3 features.\n",
    "\n",
    "\n",
    "From these, **(A) DeviceMotion_data** provides the most comprehensive set of features, combining both linear and rotational data, as well as derived orientation estimates:\n",
    "\n",
    "- **Attitude (roll, pitch, yaw)** showing device orientation (e.g., facing up/down)\n",
    "- **Gravity (x, y, z)** showing static acceleration (orientation wrt gravity)\n",
    "- **Rotation Rate (x, y, z)** showing angular velocity from gyroscope\n",
    "- **User Acceleration (x, y, z)** showing motion (dynamic body acceleration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607df06",
   "metadata": {},
   "source": [
    "** not confirmed **\n",
    "\n",
    "We opted to use data from **(A) DeviceMotion_data**, as it contains richer temporal and contextual signals necessary for activity recognition. Rather than using all 12 features, we selected a subset that strikes a balance between informativeness and model complexity by choosing only **Attitude (roll, pitch, yaw)** and **User Acceleration (x, y, z)**. Attitude provides the device's orientation, allowing for improved detection of postural changes, while acceleration is the one capturing dynamic motion and body movement which is essential for distinguishing activities like walking, sitting, or jumping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa1ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907b872",
   "metadata": {},
   "source": [
    "### Useful functions\n",
    "Here are some useful functions from motionsense.ipyb we have copied. We didn't copy get_ds_infos() since we don't really care about subject information. We edited the code to have just classified subjects [1-24] without needing to read the subject info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58907aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data_types(data_types=[\"userAcceleration\"]):\n",
    "    \"\"\"\n",
    "    Select the sensors and the mode to shape the final dataset. \n",
    "\n",
    "    Args:\n",
    "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n",
    "\n",
    "    Returns:\n",
    "        A list of columns to use for creating time-series from files.\n",
    "    \"\"\"\n",
    "    dt_list = []\n",
    "    for t in data_types:\n",
    "        if t != \"attitude\":\n",
    "            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n",
    "        else:\n",
    "            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
    "    return dt_list\n",
    "\n",
    "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
    "TRIAL_CODES = {\n",
    "    ACT_LABELS[0]:[1,2,11],\n",
    "    ACT_LABELS[1]:[3,4,12],\n",
    "    ACT_LABELS[2]:[7,8,15],\n",
    "    ACT_LABELS[3]:[9,16],\n",
    "    ACT_LABELS[4]:[6,14],\n",
    "    ACT_LABELS[5]:[5,13]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d918a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series(dt_list, act_labels, trial_codes, subject_ids=None, mode=\"mag\", labeled=True):\n",
    "    \"\"\"\n",
    "    Defines what data to include for a given set, using selected sensors and subjects.\n",
    "\n",
    "    Args:\n",
    "        dt_list: List of sensor columns to include.\n",
    "        act_labels: List of activity labels (e.g. [\"dws\", \"ups\", \"wlk\"...]).\n",
    "        trial_codes: Dictionary mapping activity to trial numbers.\n",
    "        subject_ids: List of subject IDs to include. Example: [1, 2, ..., 24]\n",
    "        mode: \"raw\" = keep all sensor components; \"mag\" = magnitude only.\n",
    "        labeled: True to include activity labels.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing time-series sensor data.\n",
    "    \"\"\"\n",
    "    if subject_ids is None:\n",
    "        subject_ids = list(range(1, 25))  # Default: use all subjects\n",
    "\n",
    "    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list * 3)\n",
    "    dataset = np.zeros((0, num_data_cols + 1)) if labeled else np.zeros((0, num_data_cols))\n",
    "\n",
    "    print(\"[INFO] -- Creating Time-Series\")\n",
    "    for sub_id in subject_ids:\n",
    "        for act_id, act in enumerate(act_labels):\n",
    "            for trial in trial_codes[act]:\n",
    "                # For Olive: fname = f'/Users/olivekschonfeldt/Library/CloudStorage/OneDrive-UniversityofCapeTown/EEE4114F DSP/ML Project 2025/motion-sense-master/data/A_DeviceMotion_data/{act}_{trial}/sub_{sub_id}.csv'\n",
    "                fname = f'/Users/olivekschonfeldt/Library/CloudStorage/OneDrive-UniversityofCapeTown/EEE4114F DSP/ML Project 2025/motion-sense-master/data/A_DeviceMotion_data/{act}_{trial}/sub_{sub_id}.csv'\n",
    "                try:\n",
    "                    raw_data = pd.read_csv(fname)\n",
    "                    raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
    "                    vals = np.zeros((len(raw_data), num_data_cols))\n",
    "                    for x_id, axes in enumerate(dt_list):\n",
    "                        if mode == \"mag\":\n",
    "                            vals[:, x_id] = (raw_data[axes] ** 2).sum(axis=1) ** 0.5\n",
    "                        else:\n",
    "                            vals[:, x_id * 3:(x_id + 1) * 3] = raw_data[axes].values\n",
    "                        vals = vals[:, :num_data_cols]\n",
    "                    if labeled:\n",
    "                        lbls = np.array([[act_id]] * len(raw_data))\n",
    "                        vals = np.concatenate((vals, lbls), axis=1)\n",
    "                    dataset = np.append(dataset, vals, axis=0)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"[WARNING] File not found: {fname}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "    cols = []\n",
    "    for axes in dt_list:\n",
    "        cols += axes if mode == \"raw\" else [str(axes[0][:-2])]\n",
    "\n",
    "    if labeled:\n",
    "        cols += [\"act\"]\n",
    "\n",
    "    dataset = pd.DataFrame(data=dataset, columns=cols)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ab224",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "Here we extract the data to obtain our new **dataset**. We are only going to extract attitude(roll, pitch, yaw) and userAcceleration(x,y,z) for all activity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Selected sensor data types: ['attitude', 'gravity', 'rotationRate', 'userAcceleration']\n",
      "[INFO] -- Selected activites: ['dws', 'ups', 'wlk', 'jog', 'std', 'sit']\n",
      "[INFO] -- Creating Time-Series\n"
     ]
    }
   ],
   "source": [
    "# Here we set parameter to build labeled time-series from dataset of \"(A)DeviceMotion_data\"\n",
    "\n",
    "sdt = [\"attitude\", \"gravity\", \"rotationRate\", \"userAcceleration\"]\n",
    "print(\"[INFO] -- Selected sensor data types: \"+str(sdt))    \n",
    "act_labels = ACT_LABELS  # includes all six activities\n",
    "print(\"[INFO] -- Selected activites: \"+str(act_labels))    \n",
    "trial_codes = {act: TRIAL_CODES[act] for act in act_labels}\n",
    "dt_list = set_data_types(sdt)\n",
    "dataset = create_time_series(dt_list, act_labels, trial_codes, mode=\"raw\", labeled=True)\n",
    "print(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19677775",
   "metadata": {},
   "source": [
    "### Understanding sample size\n",
    "Here we gain an understanding of the sample size of each activity so that we can assess the balance of the dataset. This helps identify if any classes are underrepresented, which may affect model training and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of samples per activity label\n",
    "activity_counts = dataset[\"act\"].value_counts().sort_index()\n",
    "\n",
    "# Convert numeric labels back to activity names\n",
    "activity_names = act_labels  # this must match the order of label IDs: 0, 1, 2, ...\n",
    "activity_labels = [activity_names[int(i)] for i in activity_counts.index]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(activity_labels, activity_counts.values, color='skyblue')\n",
    "plt.xlabel(\"Activity\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Size of Samples per Activity\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8ac11",
   "metadata": {},
   "source": [
    "This shows us it will be harder for the model to accurately recognise less-represented classes like downstairs, upstairs, and jogging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8923633",
   "metadata": {},
   "source": [
    "It was beneficial to plot this data because it helps us distinguish features such as:\n",
    "- Dynamic activities like walking stairs, walking and jogging have large peaks and dips, while the more conservative activities like standing and sitting have a flatter range of values.\n",
    "- Walking and jogging have a 'cadence' to them.\n",
    "\n",
    "However we have a few concerns:\n",
    "- There seems to be some 'noise' present in sitting (and what else???)\n",
    "- Walking downstairs and upstairs looks pretty similar even though they're two different directions of motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6b3d7",
   "metadata": {},
   "source": [
    "### Visualisation of Attitude Data per Activity\n",
    "Here we plot the first 600 samples per activity for the attitude data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0638ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sensor axes to plot\n",
    "sensor_axes = ['attitude.roll', 'attitude.pitch', 'attitude.yaw']\n",
    "\n",
    "for i, act_name in enumerate(act_labels):\n",
    "    subset = dataset[dataset['act'] == i].head(600)\n",
    "    plt.figure(figsize=(6, 0.5))\n",
    "    plt.title(act_name)\n",
    "    for axis in sensor_axes:\n",
    "        plt.plot(subset.index, subset[axis], label=axis)\n",
    "    plt.xticks([]) # turn off x labels\n",
    "    plt.yticks([])  # turn off y labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06b4a9",
   "metadata": {},
   "source": [
    "### Visualisation of Gravity Data per Activity\n",
    "Here we plot the first 600 samples per activity for the gravity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sensor axes to plot\n",
    "sensor_axes = ['gravity.x', 'gravity.y', 'gravity.z']\n",
    "\n",
    "for i, act_name in enumerate(act_labels):\n",
    "    subset = dataset[dataset['act'] == i].head(600)\n",
    "    plt.figure(figsize=(6, 0.5))\n",
    "    plt.title(act_name)\n",
    "    for axis in sensor_axes:\n",
    "        plt.plot(subset.index, subset[axis], label=axis)\n",
    "    plt.xticks([]) # turn off x labels\n",
    "    plt.yticks([])  # turn off y labels\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6de824",
   "metadata": {},
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf1c23",
   "metadata": {},
   "source": [
    "### Visualisation of Rotation Rate Data per Activity\n",
    "Here we plot the first 600 samples per activity for the rotation rate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20066f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sensor axes to plot\n",
    "sensor_axes = ['rotationRate.x', 'rotationRate.y', 'rotationRate.z']\n",
    "\n",
    "for i, act_name in enumerate(act_labels):\n",
    "    subset = dataset[dataset['act'] == i].head(600)\n",
    "    plt.figure(figsize=(6, 0.5))\n",
    "    plt.title(act_name)\n",
    "    for axis in sensor_axes:\n",
    "        plt.plot(subset.index, subset[axis], label=axis)\n",
    "    plt.xticks([]) # turn off x labels\n",
    "    plt.yticks([])  # turn off y labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00451fec",
   "metadata": {},
   "source": [
    "### Visualisation of Acceleration Data per Activity\n",
    "Here we plot the first 600 samples per activity for the user acceleration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sensor axes to plot\n",
    "sensor_axes = ['userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']\n",
    "\n",
    "for i, act_name in enumerate(act_labels):\n",
    "    subset = dataset[dataset['act'] == i].head(600)\n",
    "    plt.figure(figsize=(6, 0.5))\n",
    "    plt.title(act_name)\n",
    "    for axis in sensor_axes:\n",
    "        plt.plot(subset.index, subset[axis], label=axis)\n",
    "    plt.xticks([]) # turn off x labels\n",
    "    plt.yticks([])  # turn off y labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec414687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
